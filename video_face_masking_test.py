#!/usr/bin/env python3
"""
YOLOv8 Face Detection Video Masking Test
ë™ì˜ìƒì—ì„œ ì–¼êµ´ì„ ê²€ì¶œí•˜ê³  ë§ˆìŠ¤í‚¹ ì²˜ë¦¬í•˜ëŠ” í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

Requirements:
- ultralytics
- huggingface_hub
- opencv-python
- numpy
- supervision
"""

import cv2
import numpy as np
import os
import argparse
from pathlib import Path
from huggingface_hub import hf_hub_download
from ultralytics import YOLO
import time
from PIL import Image, ImageDraw, ImageFont
import platform
import subprocess
import shutil
import tempfile

class VideoFaceMasking:
    def __init__(self, confidence_threshold=0.15):
        """
        YOLOv8 ì–¼êµ´ ë§ˆìŠ¤í‚¹ í´ë˜ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            confidence_threshold (float): ì‹ ë¢°ë„ ì„ê³„ê°’ (ê¸°ë³¸ 0.15)
        """
        self.confidence_threshold = confidence_threshold
        self.model = None
        self.font = self.load_korean_font()
        
        # ë§ˆìŠ¤í‚¹ ì—°ì†ì„±ì„ ìœ„í•œ ì–¼êµ´ ì¶”ì 
        self.face_tracks = []  # ì¶”ì  ì¤‘ì¸ ì–¼êµ´ë“¤
        self.track_id_counter = 0  # ì¶”ì  ID ì¹´ìš´í„°
        self.max_missing_frames = 10  # ìµœëŒ€ ë†“ì¹œ í”„ë ˆì„ ìˆ˜
        self.iou_threshold = 0.3  # IoU ì„ê³„ê°’
        self.distance_threshold = 100  # ì¤‘ì‹¬ì  ê±°ë¦¬ ì„ê³„ê°’ (í”½ì…€)
        self.size_ratio_threshold = 0.5  # í¬ê¸° ë³€í™” ì„ê³„ê°’
        
        self.load_model()
    
    def load_korean_font(self):
        """í•œê¸€ ì§€ì› í°íŠ¸ ë¡œë“œ"""
        try:
            if platform.system() == "Windows":
                # Windows ê¸°ë³¸ í•œê¸€ í°íŠ¸ë“¤ ì‹œë„
                font_paths = [
                    "C:/Windows/Fonts/malgun.ttf",  # ë§‘ì€ ê³ ë”•
                    "C:/Windows/Fonts/gulim.ttc",   # êµ´ë¦¼
                    "C:/Windows/Fonts/batang.ttc"   # ë°”íƒ•
                ]
                for font_path in font_paths:
                    if os.path.exists(font_path):
                        return ImageFont.truetype(font_path, 30)
            elif platform.system() == "Darwin":  # macOS
                return ImageFont.truetype("/System/Library/Fonts/AppleSDGothicNeo.ttc", 30)
            else:  # Linux
                # Linuxì˜ ê²½ìš° Noto Sans CJK ì‹œë„
                font_paths = [
                    "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
                    "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc"
                ]
                for font_path in font_paths:
                    if os.path.exists(font_path):
                        return ImageFont.truetype(font_path, 30)
        except Exception as e:
            print(f"âš ï¸  í•œê¸€ í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ê¸°ë³¸ í°íŠ¸ ì‚¬ìš© (í•œê¸€ ê¹¨ì§ˆ ìˆ˜ ìˆìŒ)
        try:
            return ImageFont.load_default()
        except:
            return None
    
    def load_model(self):
        """HuggingFaceì—ì„œ YOLOv8-Face-Detection ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ"""
        try:
            print("ğŸ“¥ YOLOv8-Face-Detection ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            model_path = hf_hub_download(
                repo_id="arnabdhar/YOLOv8-Face-Detection", 
                filename="model.pt"
            )
            print(f"âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {model_path}")
            
            # YOLOv8 ëª¨ë¸ ë¡œë“œ
            self.model = YOLO(model_path)
            print("âœ… YOLOv8 ì–¼êµ´ ê²€ì¶œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def calculate_iou(self, box1, box2):
        """
        ë‘ ë°•ìŠ¤ ê°„ì˜ IoU(Intersection over Union) ê³„ì‚°
        """
        x1_1, y1_1, x2_1, y2_1 = box1[:4]
        x1_2, y1_2, x2_2, y2_2 = box2[:4]
        
        # êµì§‘í•© ì˜ì—­ ê³„ì‚°
        x1_i = max(x1_1, x1_2)
        y1_i = max(y1_1, y1_2)
        x2_i = min(x2_1, x2_2)
        y2_i = min(y2_1, y2_2)
        
        if x2_i <= x1_i or y2_i <= y1_i:
            return 0.0
            
        intersection = (x2_i - x1_i) * (y2_i - y1_i)
        
        # í•©ì§‘í•© ì˜ì—­ ê³„ì‚°
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0

    def calculate_center_distance(self, box1, box2):
        """
        ë‘ ë°•ìŠ¤ì˜ ì¤‘ì‹¬ì  ê°„ ê±°ë¦¬ ê³„ì‚°
        """
        x1_1, y1_1, x2_1, y2_1 = box1[:4]
        x1_2, y1_2, x2_2, y2_2 = box2[:4]
        
        center1_x = (x1_1 + x2_1) / 2
        center1_y = (y1_1 + y2_1) / 2
        center2_x = (x1_2 + x2_2) / 2
        center2_y = (y1_2 + y2_2) / 2
        
        distance = ((center1_x - center2_x) ** 2 + (center1_y - center2_y) ** 2) ** 0.5
        return distance
    
    def calculate_size_ratio(self, box1, box2):
        """
        ë‘ ë°•ìŠ¤ì˜ í¬ê¸° ë¹„ìœ¨ ê³„ì‚°
        """
        x1_1, y1_1, x2_1, y2_1 = box1[:4]
        x1_2, y1_2, x2_2, y2_2 = box2[:4]
        
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        
        if area1 == 0 or area2 == 0:
            return 0
            
        ratio = min(area1, area2) / max(area1, area2)
        return ratio
    
    def calculate_similarity(self, track_box, detection_box):
        """
        ì¢…í•©ì ì¸ ìœ ì‚¬ë„ ê³„ì‚° (IoU + ê±°ë¦¬ + í¬ê¸°)
        """
        iou = self.calculate_iou(track_box, detection_box)
        distance = self.calculate_center_distance(track_box, detection_box)
        size_ratio = self.calculate_size_ratio(track_box, detection_box)
        
        # ê±°ë¦¬ ì ìˆ˜ (ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
        distance_score = max(0, 1 - distance / self.distance_threshold)
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
        similarity = (iou * 0.4 + distance_score * 0.4 + size_ratio * 0.2)
        
        return similarity

    def detect_faces(self, frame):
        """
        í”„ë ˆì„ì—ì„œ ì–¼êµ´ ê²€ì¶œ
        
        Args:
            frame: OpenCV ì´ë¯¸ì§€ í”„ë ˆì„
            
        Returns:
            list: ê²€ì¶œëœ ì–¼êµ´ ë°•ìŠ¤ ë¦¬ìŠ¤íŠ¸ [(x1, y1, x2, y2, confidence), ...]
        """
        try:
            # YOLOv8 ì¶”ë¡ 
            results = self.model(frame, verbose=False)
            
            faces = []
            for result in results:
                boxes = result.boxes
                if boxes is not None:
                    for box in boxes:
                        # ì‹ ë¢°ë„ í™•ì¸
                        confidence = float(box.conf[0])
                        if confidence >= self.confidence_threshold:
                            # ì¢Œí‘œ ì¶”ì¶œ (x1, y1, x2, y2)
                            x1, y1, x2, y2 = map(int, box.xyxy[0])
                            faces.append((x1, y1, x2, y2, confidence))
            
            return faces
            
        except Exception as e:
            print(f"âŒ ì–¼êµ´ ê²€ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def update_face_tracking(self, detected_faces):
        """
        ì–¼êµ´ ì¶”ì  ì—…ë°ì´íŠ¸ (ì—°ì†ì„± ê°œì„ )
        """
        # ìƒˆë¡œìš´ ì¶”ì  ë¦¬ìŠ¤íŠ¸
        updated_tracks = []
        used_detections = set()
        
        # ê¸°ì¡´ ì¶”ì ê³¼ ìƒˆ ê²€ì¶œ ë§¤ì¹­ (ê³ ê¸‰ ìœ ì‚¬ë„ ê¸°ë°˜)
        for track in self.face_tracks:
            best_match = None
            best_similarity = 0
            best_idx = -1
            
            for i, detection in enumerate(detected_faces):
                if i in used_detections:
                    continue
                    
                # ì¢…í•©ì ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = self.calculate_similarity(track['box'], detection)
                
                # ìµœì†Œ ì„ê³„ê°’ ì¡°ê±´ë“¤
                distance = self.calculate_center_distance(track['box'], detection)
                size_ratio = self.calculate_size_ratio(track['box'], detection)
                
                # ë” ê´€ëŒ€í•œ ì¡°ê±´ìœ¼ë¡œ ë¹ ë¥¸ ì›€ì§ì„ í—ˆìš©
                if (similarity > 0.2 and  # ì „ì²´ ìœ ì‚¬ë„
                    distance < self.distance_threshold and  # ê±°ë¦¬ ì œí•œ
                    size_ratio > self.size_ratio_threshold and  # í¬ê¸° ë³€í™” ì œí•œ
                    similarity > best_similarity):
                    best_similarity = similarity
                    best_match = detection
                    best_idx = i
            
            if best_match is not None:
                # ê¸°ì¡´ ì¶”ì  ì—…ë°ì´íŠ¸
                track['box'] = best_match
                track['missing_count'] = 0
                track['confidence'] = best_match[4]
                updated_tracks.append(track)
                used_detections.add(best_idx)
            else:
                # ë§¤ì¹­ë˜ì§€ ì•Šì€ ì¶”ì  - ë†“ì¹œ íšŸìˆ˜ ì¦ê°€
                track['missing_count'] += 1
                if track['missing_count'] < self.max_missing_frames:
                    # ì´ì „ ìœ„ì¹˜ ìœ ì§€ (ì—°ì†ì„±)
                    updated_tracks.append(track)
        
        # ìƒˆë¡œìš´ ê²€ì¶œ ì¶”ê°€
        for i, detection in enumerate(detected_faces):
            if i not in used_detections:
                new_track = {
                    'id': self.track_id_counter,
                    'box': detection,
                    'missing_count': 0,
                    'confidence': detection[4]
                }
                updated_tracks.append(new_track)
                self.track_id_counter += 1
        
        # ì¤‘ë³µëœ ì¶”ì  ì œê±° (ê°™ì€ ì‚¬ëŒì„ ì—¬ëŸ¬ ë²ˆ ì¶”ì í•˜ëŠ” ê²ƒ ë°©ì§€)
        final_tracks = self.remove_duplicate_tracks(updated_tracks)
        self.face_tracks = final_tracks
        
        # ì¶”ì ëœ ì–¼êµ´ ë°•ìŠ¤ë“¤ ë°˜í™˜
        return [track['box'] for track in self.face_tracks]
    
    def remove_duplicate_tracks(self, tracks):
        """
        ì¤‘ë³µëœ ì¶”ì  ì œê±° (ê°™ì€ ì‚¬ëŒì„ ì—¬ëŸ¬ ê°œë¡œ ì¸ì‹í•˜ëŠ” ê²ƒ ë°©ì§€)
        """
        if len(tracks) <= 1:
            return tracks
            
        final_tracks = []
        
        for i, track1 in enumerate(tracks):
            is_duplicate = False
            
            for j, track2 in enumerate(final_tracks):
                # ì´ë¯¸ ì¶”ê°€ëœ ì¶”ì ê³¼ ë¹„êµ
                similarity = self.calculate_similarity(track1['box'], track2['box'])
                distance = self.calculate_center_distance(track1['box'], track2['box'])
                
                # ë„ˆë¬´ ë¹„ìŠ·í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
                if similarity > 0.7 or distance < 50:
                    is_duplicate = True
                    # ë” ì‹ ë¢°ë„ ë†’ì€ ê²ƒìœ¼ë¡œ ìœ ì§€
                    if track1['confidence'] > track2['confidence']:
                        final_tracks[j] = track1
                    break
            
            if not is_duplicate:
                final_tracks.append(track1)
        
        return final_tracks
    

    
    def add_person_count_text(self, frame, person_count):
        """
        í”„ë ˆì„ ì¢Œì¸¡ ìƒë‹¨ì— í•œê¸€ë¡œ ì¸ì› ìˆ˜ í‘œì‹œ
        
        Args:
            frame: OpenCV ì´ë¯¸ì§€ í”„ë ˆì„
            person_count: ê²€ì¶œëœ ì¸ì› ìˆ˜
            
        Returns:
            numpy.ndarray: í…ìŠ¤íŠ¸ê°€ ì¶”ê°€ëœ í”„ë ˆì„
        """
        try:
            # OpenCV í”„ë ˆì„ì„ PIL Imageë¡œ ë³€í™˜
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            draw = ImageDraw.Draw(pil_image)
            
            # í…ìŠ¤íŠ¸ ë‚´ìš©
            text = f"ì¸ì›: {person_count}ëª…"
            
            # í…ìŠ¤íŠ¸ ìœ„ì¹˜ (ì¢Œì¸¡ ìƒë‹¨)
            text_position = (20, 20)
            
            # í…ìŠ¤íŠ¸ ë°°ê²½ì„ ìœ„í•œ ë°˜íˆ¬ëª… ë°•ìŠ¤
            if self.font:
                # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
                bbox = draw.textbbox(text_position, text, font=self.font)
                text_width = bbox[2] - bbox[0]
                text_height = bbox[3] - bbox[1]
            else:
                # í°íŠ¸ê°€ ì—†ì„ ê²½ìš° ëŒ€ëµì ì¸ í¬ê¸°
                text_width = len(text) * 15
                text_height = 25
            
            # ë°°ê²½ ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ë°˜íˆ¬ëª… ê²€ì€ìƒ‰)
            background_box = [
                text_position[0] - 10,
                text_position[1] - 5,
                text_position[0] + text_width + 10,
                text_position[1] + text_height + 5
            ]
            
            # ë°˜íˆ¬ëª… ë°°ê²½
            overlay = Image.new('RGBA', pil_image.size, (0, 0, 0, 0))
            overlay_draw = ImageDraw.Draw(overlay)
            overlay_draw.rectangle(background_box, fill=(0, 0, 0, 128))  # ë°˜íˆ¬ëª… ê²€ì€ìƒ‰
            
            # ë°°ê²½ê³¼ ì›ë³¸ ì´ë¯¸ì§€ í•©ì„±
            pil_image = Image.alpha_composite(pil_image.convert('RGBA'), overlay).convert('RGB')
            draw = ImageDraw.Draw(pil_image)
            
            # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸° (í°ìƒ‰)
            if self.font:
                draw.text(text_position, text, font=self.font, fill=(255, 255, 255))
            else:
                # í°íŠ¸ê°€ ì—†ì„ ê²½ìš° ê¸°ë³¸ í°íŠ¸ (ì˜ì–´/ìˆ«ìë§Œ í‘œì‹œ)
                draw.text(text_position, f"Count: {person_count}", fill=(255, 255, 255))
            
            # PIL Imageë¥¼ ë‹¤ì‹œ OpenCV í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
            frame_with_text = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
            
            return frame_with_text
            
        except Exception as e:
            print(f"âš ï¸  í…ìŠ¤íŠ¸ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ OpenCVë¡œ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ê°€
            cv2.putText(frame, f"Count: {person_count}", (20, 50), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)
            return frame

    def apply_face_masking(self, frame, faces, mask_type="blur"):
        """
        ê²€ì¶œëœ ì–¼êµ´ì— ë§ˆìŠ¤í‚¹ ì ìš©
        
        Args:
            frame: OpenCV ì´ë¯¸ì§€ í”„ë ˆì„
            faces: ê²€ì¶œëœ ì–¼êµ´ ë°•ìŠ¤ ë¦¬ìŠ¤íŠ¸
            mask_type: ë§ˆìŠ¤í‚¹ íƒ€ì… ("blur", "pixelate", "black")
            
        Returns:
            numpy.ndarray: ë§ˆìŠ¤í‚¹ëœ í”„ë ˆì„
        """
        masked_frame = frame.copy()
        
        for face in faces:
            x1, y1, x2, y2, confidence = face
            
            # ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ
            face_region = masked_frame[y1:y2, x1:x2]
            
            if face_region.size == 0:
                continue
                
            # ë§ˆìŠ¤í‚¹ íƒ€ì…ë³„ ì²˜ë¦¬
            if mask_type == "blur":
                # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì ìš© (ê°•ë„ ë†’ì„)
                blurred = cv2.GaussianBlur(face_region, (51, 51), 30)
                masked_frame[y1:y2, x1:x2] = blurred
                
            elif mask_type == "pixelate":
                # í”½ì…€í™” íš¨ê³¼
                h, w = face_region.shape[:2]
                temp = cv2.resize(face_region, (w//10, h//10), interpolation=cv2.INTER_LINEAR)
                pixelated = cv2.resize(temp, (w, h), interpolation=cv2.INTER_NEAREST)
                masked_frame[y1:y2, x1:x2] = pixelated
                
            elif mask_type == "black":
                # ê²€ì€ìƒ‰ ë°•ìŠ¤ë¡œ ë§ˆìŠ¤í‚¹
                masked_frame[y1:y2, x1:x2] = 0
            
            # ë””ë²„ê¹…ìš©: ì‹ ë¢°ë„ í‘œì‹œ (ì„ íƒì‚¬í•­)
            # cv2.putText(masked_frame, f'{confidence:.2f}', (x1, y1-10), 
            #            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        
        return masked_frame
    
    def process_video(self, input_path, output_path, mask_type="blur", show_progress=True):
        """
        ë™ì˜ìƒ íŒŒì¼ ì „ì²´ ì²˜ë¦¬
        
        Args:
            input_path (str): ì…ë ¥ ë™ì˜ìƒ ê²½ë¡œ
            output_path (str): ì¶œë ¥ ë™ì˜ìƒ ê²½ë¡œ
            mask_type (str): ë§ˆìŠ¤í‚¹ íƒ€ì…
            show_progress (bool): ì§„í–‰ë¥  í‘œì‹œ ì—¬ë¶€
        """
        if not os.path.exists(input_path):
            raise FileNotFoundError(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
        
        # ë¹„ë””ì˜¤ ìº¡ì²˜ ê°ì²´ ìƒì„±
        cap = cv2.VideoCapture(input_path)
        
        # ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        print(f"ğŸ“¹ ì…ë ¥ ë™ì˜ìƒ ì •ë³´:")
        print(f"   - í•´ìƒë„: {width}x{height}")
        print(f"   - FPS: {fps}")
        print(f"   - ì´ í”„ë ˆì„: {total_frames}")
        print(f"   - ì‹ ë¢°ë„ ì„ê³„ê°’: {self.confidence_threshold}")
        
        # ë¹„ë””ì˜¤ ë¼ì´í„° ê°ì²´ ìƒì„± (ì„ì‹œ íŒŒì¼, ìŒì„± ì—†ìŒ)
        temp_video = tempfile.mktemp(suffix='_temp.mp4')
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(temp_video, fourcc, fps, (width, height))
        
        frame_count = 0
        start_time = time.time()
        
        print(f"ğŸ¬ ë™ì˜ìƒ ì²˜ë¦¬ ì‹œì‘...")
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # ì–¼êµ´ ê²€ì¶œ
                detected_faces = self.detect_faces(frame)
                
                # ì–¼êµ´ ì¶”ì ìœ¼ë¡œ ì—°ì†ì„± ê°œì„ 
                tracked_faces = self.update_face_tracking(detected_faces)
                
                # ë§ˆìŠ¤í‚¹ ì ìš©
                masked_frame = self.apply_face_masking(frame, tracked_faces, mask_type)
                
                # ì¸ì› ìˆ˜ í…ìŠ¤íŠ¸ ì¶”ê°€
                final_frame = self.add_person_count_text(masked_frame, len(tracked_faces))
                
                # í”„ë ˆì„ ì €ì¥
                out.write(final_frame)
                
                frame_count += 1
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if show_progress and frame_count % 30 == 0:  # 30í”„ë ˆì„ë§ˆë‹¤ í‘œì‹œ
                    progress = (frame_count / total_frames) * 100
                    elapsed_time = time.time() - start_time
                    avg_fps = frame_count / elapsed_time if elapsed_time > 0 else 0
                    
                    print(f"   ì§„í–‰ë¥ : {progress:.1f}% ({frame_count}/{total_frames}) "
                          f"- ì²˜ë¦¬ ì†ë„: {avg_fps:.1f} FPS - í˜„ì¬ ì¸ì›: {len(tracked_faces)}ëª… "
                          f"(ê²€ì¶œ: {len(detected_faces)}, ì¶”ì : {len(tracked_faces)})")
        
        finally:
            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            cap.release()
            out.release()
            cv2.destroyAllWindows()
        
        # ì›ë³¸ ìŒì„±ê³¼ ë§ˆìŠ¤í‚¹ëœ ë¹„ë””ì˜¤ í•©ì¹˜ê¸°
        print("ğŸ”Š ì›ë³¸ ìŒì„±ê³¼ ë§ˆìŠ¤í‚¹ëœ ë¹„ë””ì˜¤ í•©ì¹˜ëŠ” ì¤‘...")
        
        merge_command = [
            'ffmpeg', '-i', temp_video,  # ë§ˆìŠ¤í‚¹ëœ ë¹„ë””ì˜¤ (ìŒì„± ì—†ìŒ)
            '-i', input_path,  # ì›ë³¸ ë¹„ë””ì˜¤ (ìŒì„± í¬í•¨)
            '-c:v', 'copy',  # ë¹„ë””ì˜¤ ì½”ë± ë³µì‚¬ (ë¹ ë¥¸ ì²˜ë¦¬)
            '-c:a', 'aac',   # ì˜¤ë””ì˜¤ ì½”ë± AAC
            '-map', '0:v:0',  # ì²« ë²ˆì§¸ ì…ë ¥ì˜ ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼
            '-map', '1:a:0',  # ë‘ ë²ˆì§¸ ì…ë ¥ì˜ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼
            '-shortest',  # ì§§ì€ ìª½ì— ë§ì¶¤
            '-y',  # ë®ì–´ì“°ê¸°
            output_path
        ]
        
        try:
            subprocess.run(merge_command, capture_output=True, text=True, check=True)
            print("âœ… ìŒì„±ê³¼ ë¹„ë””ì˜¤ í•©ì¹˜ê¸° ì™„ë£Œ!")
        except subprocess.CalledProcessError as e:
            print(f"âš ï¸ ìŒì„± í•©ì¹˜ê¸° ì‹¤íŒ¨, ëŒ€ì²´ ë°©ë²• ì‹œë„...")
            print(f"ì˜¤ë¥˜: {e.stderr}")
            
            # ëŒ€ì²´ ë°©ë²•: ë” í˜¸í™˜ì„± ë†’ì€ ëª…ë ¹ì–´
            fallback_command = [
                'ffmpeg', '-i', temp_video,
                '-i', input_path,
                '-c:v', 'libx264',  # H.264 ì½”ë± ì‚¬ìš©
                '-c:a', 'aac',
                '-map', '0:v',  # ë§ˆìŠ¤í‚¹ëœ ë¹„ë””ì˜¤ì˜ ì˜ìƒ
                '-map', '1:a',  # ì›ë³¸ì˜ ìŒì„±
                '-shortest',
                '-y',
                output_path
            ]
            
            try:
                subprocess.run(fallback_command, capture_output=True, text=True, check=True)
                print("âœ… ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ìŒì„±ê³¼ ë¹„ë””ì˜¤ í•©ì¹˜ê¸° ì™„ë£Œ!")
            except subprocess.CalledProcessError as e2:
                print(f"âŒ ìŒì„± í•©ì¹˜ê¸° ìµœì¢… ì‹¤íŒ¨: {e2.stderr}")
                # ë§ˆìŠ¤í‚¹ë§Œ ìˆëŠ” ë¹„ë””ì˜¤ë¼ë„ ì €ì¥
                shutil.move(temp_video, output_path)
                print(f"âš ï¸ ìŒì„± ì—†ì´ ë§ˆìŠ¤í‚¹ë§Œ ì €ì¥ë¨: {output_path}")
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        try:
            os.unlink(temp_video)
        except:
            pass
        
        processing_time = time.time() - start_time
        print(f"âœ… ë™ì˜ìƒ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   - ì´ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
        print(f"   - í‰ê·  ì²˜ë¦¬ ì†ë„: {frame_count/processing_time:.2f} FPS")
        print(f"   - ì¶œë ¥ íŒŒì¼: {output_path} (ìŒì„± í¬í•¨)")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='YOLOv8 ì–¼êµ´ ë§ˆìŠ¤í‚¹ ë™ì˜ìƒ ì²˜ë¦¬')
    parser.add_argument('--input', '-i', type=str, default='cctv_fast.mp4',
                       help='ì…ë ¥ ë™ì˜ìƒ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: cctv_fast.mp4)')
    parser.add_argument('--output', '-o', type=str, default='mask_cctv_fast.mp4',
                       help='ì¶œë ¥ ë™ì˜ìƒ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: mask_cctv_fast.mp4)')
    parser.add_argument('--confidence', '-c', type=float, default=0.15,
                       help='ì‹ ë¢°ë„ ì„ê³„ê°’ (ê¸°ë³¸: 0.15)')
    parser.add_argument('--mask-type', '-m', type=str, default='blur',
                       choices=['blur', 'pixelate', 'black'],
                       help='ë§ˆìŠ¤í‚¹ íƒ€ì… (ê¸°ë³¸: blur)')
    
    args = parser.parse_args()
    
    try:
        # ì–¼êµ´ ë§ˆìŠ¤í‚¹ ê°ì²´ ìƒì„±
        face_masker = VideoFaceMasking(confidence_threshold=args.confidence)
        
        # ë™ì˜ìƒ ì²˜ë¦¬
        face_masker.process_video(
            input_path=args.input,
            output_path=args.output,
            mask_type=args.mask_type
        )
        
    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 