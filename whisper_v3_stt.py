#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FaceMaker STT with Whisper-Large-V3
HuggingFace ê³µì‹ whisper-large-v3 ëª¨ë¸ ì‚¬ìš©
"""

import cv2
import torch
import numpy as np
import argparse
import os
from pathlib import Path
import subprocess
import tempfile
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
from PIL import Image, ImageDraw, ImageFont
import warnings
warnings.filterwarnings("ignore")

class WhisperV3STT:
    def __init__(self):
        """
        Whisper-Large-V3 ê¸°ë°˜ ë‹¤êµ­ì–´ STT í´ë˜ìŠ¤
        ì§€ì› ì–¸ì–´: í•œêµ­ì–´, ì¤‘êµ­ì–´, ì˜ì–´ ìë™ ê°ì§€
        """
        print("ğŸš€ Whisper-Large-V3 ë‹¤êµ­ì–´ ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        self.device = "cuda:0" if torch.cuda.is_available() else "cpu"
        self.torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        
        print(f"ğŸ’» ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"ğŸ”¢ ë°ì´í„° íƒ€ì…: {self.torch_dtype}")
        
        # ëª¨ë¸ ID
        self.model_id = "openai/whisper-large-v3"
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = AutoModelForSpeechSeq2Seq.from_pretrained(
            self.model_id, 
            torch_dtype=self.torch_dtype, 
            low_cpu_mem_usage=True, 
            use_safetensors=True
        )
        self.model.to(self.device)
        
        # í”„ë¡œì„¸ì„œ ë¡œë“œ
        self.processor = AutoProcessor.from_pretrained(self.model_id)
        
        # íŒŒì´í”„ë¼ì¸ ìƒì„±
        self.pipe = pipeline(
            "automatic-speech-recognition",
            model=self.model,
            tokenizer=self.processor.tokenizer,
            feature_extractor=self.processor.feature_extractor,
            torch_dtype=self.torch_dtype,
            device=self.device,
        )
        
        print("âœ… Whisper-Large-V3 ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    def extract_audio_with_ffmpeg(self, video_path, output_path=None):
        """
        FFmpegë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ
        """
        if output_path is None:
            output_path = video_path.replace('.mp4', '_audio.wav')
        
        print(f"ğŸµ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘: {video_path} -> {output_path}")
        
        command = [
            'ffmpeg', '-i', video_path,
            '-vn',  # ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ë¬´ì‹œ
            '-acodec', 'pcm_s16le',  # 16ë¹„íŠ¸ PCM
            '-ar', '16000',  # 16kHz ìƒ˜í”Œë§
            '-ac', '1',  # ëª¨ë…¸ ì±„ë„
            '-y',  # ë®ì–´ì“°ê¸°
            output_path
        ]
        
        try:
            result = subprocess.run(command, capture_output=True, text=True, check=True)
            print("âœ… ì˜¤ë””ì˜¤ ì¶”ì¶œ ì™„ë£Œ!")
            return output_path
        except subprocess.CalledProcessError as e:
            print(f"âŒ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            print(f"stdout: {e.stdout}")
            print(f"stderr: {e.stderr}")
            return None
    
    def split_audio_chunks(self, audio_path, chunk_duration=30):
        """
        ì˜¤ë””ì˜¤ë¥¼ ì²­í¬ë¡œ ë¶„í•  (whisperì˜ ìµœëŒ€ ì…ë ¥ ê¸¸ì´: 30ì´ˆ)
        """
        print(f"âœ‚ï¸ ì˜¤ë””ì˜¤ ì²­í¬ ë¶„í•  ì¤‘... (ì²­í¬ ê¸¸ì´: {chunk_duration}ì´ˆ)")
        
        # ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸
        command = [
            'ffprobe', '-v', 'quiet', '-show_entries', 'format=duration',
            '-of', 'csv=p=0', audio_path
        ]
        
        try:
            result = subprocess.run(command, capture_output=True, text=True, check=True)
            duration = float(result.stdout.strip())
            print(f"ğŸ“ ì´ ì˜¤ë””ì˜¤ ê¸¸ì´: {duration:.2f}ì´ˆ")
        except:
            print("âš ï¸ ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")
            duration = 300  # 5ë¶„ìœ¼ë¡œ ê°€ì •
        
        # ì²­í¬ ë¶„í• 
        chunks = []
        temp_dir = tempfile.mkdtemp()
        
        for i, start_time in enumerate(range(0, int(duration), chunk_duration)):
            chunk_path = os.path.join(temp_dir, f"chunk_{i:03d}.wav")
            
            command = [
                'ffmpeg', '-i', audio_path,
                '-ss', str(start_time),
                '-t', str(chunk_duration),
                '-acodec', 'pcm_s16le',
                '-ar', '16000',
                '-ac', '1',
                '-y', chunk_path
            ]
            
            try:
                subprocess.run(command, capture_output=True, check=True)
                chunks.append((chunk_path, start_time, min(start_time + chunk_duration, duration)))
                print(f"ğŸ“¦ ì²­í¬ {i+1} ìƒì„±: {start_time}~{min(start_time + chunk_duration, duration)}ì´ˆ")
            except subprocess.CalledProcessError:
                print(f"âŒ ì²­í¬ {i+1} ìƒì„± ì‹¤íŒ¨")
        
        return chunks
    
    def transcribe_chunk(self, audio_path, chunk_start_time):
        """
        ë‹¨ì¼ ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ì „ì‚¬ (ë¬¸ì¥ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
        """
        try:
            # ë¬¸ì¥ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ì„¤ì • (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ ê°•í™”)
            generate_kwargs = {
                "max_new_tokens": 224,
                "num_beams": 1,
                "condition_on_prev_tokens": False,
                "compression_ratio_threshold": 1.35,
                "temperature": (0.0, 0.2, 0.4),  # 3ë‹¨ê³„ë¡œ ì¤„ì—¬ì„œ ì†ë„â†‘, ì •í™•ë„ ìœ ì§€
                "logprob_threshold": -0.8,  # -1.0 -> -0.8ë¡œ ë” ì—„ê²©í•˜ê²Œ
                "no_speech_threshold": 0.6,  # 0.5 -> 0.6ìœ¼ë¡œ ë” ì—„ê²©í•˜ê²Œ
                "task": "transcribe"
            }
            
            # ë¬¸ì¥ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ì–»ê¸°
            result = self.pipe(audio_path, return_timestamps=True, generate_kwargs=generate_kwargs)
            
            if result and 'chunks' in result:
                # ë¬¸ì¥ë³„ë¡œ ë¶„ë¦¬ëœ ê²°ê³¼ ì²˜ë¦¬
                sentences = []
                for chunk in result['chunks']:
                    text = chunk['text'].strip()
                    if len(text) > 3 and not self._is_hallucination(text):
                        # ë¬¸ì¥ ì¢…ë£Œ ì‹œì ìœ¼ë¡œ íƒ€ì„ìŠ¤íƒ¬í”„ ì„¤ì • (ì‹¤ì‹œê°„ ëŠë‚Œ)
                        start_time = chunk_start_time + chunk['timestamp'][0]
                        end_time = chunk_start_time + chunk['timestamp'][1]
                        
                        # ìë§‰ í‘œì‹œ ì§€ì—°: ë§ì´ ëë‚˜ëŠ” ì‹œì  + 0.5ì´ˆ í›„ì— í‘œì‹œ
                        display_time = end_time + 0.5
                        
                        sentences.append({
                            'timestamp': display_time,
                            'text': text,
                            'speech_start': start_time,
                            'speech_end': end_time
                        })
                return sentences
            
            return []
            
        except Exception as e:
            print(f"âš ï¸ ì „ì‚¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _is_hallucination(self, text):
        """
        í™˜ê° í…ìŠ¤íŠ¸ ê°ì§€ (ë‹¤êµ­ì–´ íŒ¨í„´ ë§¤ì¹­)
        """
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì²´í¬ (ë„ˆë¬´ ì§§ê±°ë‚˜ ë„ˆë¬´ ê¸¸ë©´ ì˜ì‹¬)
        if len(text.strip()) < 2 or len(text.strip()) > 200:
            return True
            
        # í•œêµ­ì–´/ì¤‘êµ­ì–´/ì˜ì–´ í™˜ê° íŒ¨í„´
        hallucination_patterns = [
            # í•œêµ­ì–´ í™˜ê° (ë” ë§ì´ ì¶”ê°€)
            "ì´ì œëŠ”", "ì§€ê¸ˆì€", "ê·¸ë ‡ë‹¤ë©´", "ê·¸ëŸ°ë°", "í•˜ì§€ë§Œ", "ê·¸ë˜ì„œ", "ê·¸ë¦¬ê³ ",
            "ì‹ ì´", "ì•½ê°„", "ë˜ë‚˜", "ì´ê²¼", "ì¼ì–´ë‚ ", "ìˆ˜ ìˆì–´", "ê·¸ë˜",
            "ì–´ë””", "ë­ì•¼", "ì•„ë‹ˆ", "ë§ì•„", "ê·¸ëƒ¥", "ê·¼ë°", "ì§„ì§œ",
            # ì˜ì–´ í™˜ê°
            "thanks for watching", "subscribe", "like", "comment", "share",
            "please subscribe", "hit the bell", "notification",
            "you know", "i think", "right now", "okay okay",
            # ì¤‘êµ­ì–´ í™˜ê°
            "è°¢è°¢è§‚çœ‹", "è¯·è®¢é˜…", "ç‚¹èµ", "è¯„è®º", "åˆ†äº«", "æ„Ÿè°¢æ”¶çœ‹"
        ]
        
        text_lower = text.lower().strip()
        
        # íŒ¨í„´ ë§¤ì¹­
        for pattern in hallucination_patterns:
            if pattern in text_lower:
                return True
        
        # ë°˜ë³µë˜ëŠ” ë‹¨ì–´ ê²€ì‚¬ (ê°™ì€ ë‹¨ì–´ê°€ ì—¬ëŸ¬ë²ˆ)
        words = text.split()
        if len(words) > 1:
            unique_words = set(words)
            if len(unique_words) < len(words) * 0.5:  # 50% ì´ìƒ ì¤‘ë³µ
                return True
        
        # ì˜ë¯¸ì—†ëŠ” ì§§ì€ ë¬¸ì¥ ê²€ì‚¬
        meaningless_shorts = ["ì•„", "ì–´", "ìŒ", "ì˜¤", "ë„¤", "ì˜ˆ", "ì‘", "ok", "okay"]
        if text_lower in meaningless_shorts:
            return True
            
        # ë°˜ë³µ íŒ¨í„´ ê²€ì‚¬
        if len(words) > 2:
            if len(set(words)) < len(words) * 0.5:  # 50% ì´ìƒ ì¤‘ë³µ
                return True
        
        return False
    
    def get_korean_font(self, size=24):
        """
        í•œêµ­ì–´ í°íŠ¸ ë¡œë“œ
        """
        font_paths = [
            "C:/Windows/Fonts/malgun.ttf",  # Windows ë§‘ì€ê³ ë”•
            "C:/Windows/Fonts/gulim.ttc",   # Windows êµ´ë¦¼
            "/System/Library/Fonts/AppleSDGothicNeo.ttc",  # macOS
            "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",  # Linux
        ]
        
        for font_path in font_paths:
            if os.path.exists(font_path):
                try:
                    return ImageFont.truetype(font_path, size)
                except:
                    continue
        
        # ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©
        try:
            return ImageFont.load_default()
        except:
            return None
    
    def add_subtitle_to_frame(self, frame, subtitle_lines, max_lines=10):
        """
        í”„ë ˆì„ ìš°ì¸¡ì— ì±„íŒ…ì°½ ìŠ¤íƒ€ì¼ ìë§‰ ì¶”ê°€
        """
        height, width = frame.shape[:2]
        
        # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
        frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(frame_pil)
        
        # í°íŠ¸ ì„¤ì •
        font = self.get_korean_font(size=20)
        
        # ìš°ì¸¡ ì˜ì—­ ì„¤ì • (í¬ê¸° ì¶•ì†Œ)
        subtitle_width = width // 6  # 1/3 -> 1/6ìœ¼ë¡œ ì¶•ì†Œ
        subtitle_x = width - subtitle_width - 10
        
        # ë°°ê²½ ì˜ì—­ ê·¸ë¦¬ê¸° (íˆ¬ëª…ë„ ì¦ê°€, ë†’ì´ ë” í¬ê²Œ)
        bg_height = int(height * 0.8)  # ë†’ì´ë¥¼ 80%ë¡œ ë” í¬ê²Œ
        bg_rect = [subtitle_x - 5, 10, width - 5, 10 + bg_height]
        
        # ë°˜íˆ¬ëª… ê²€ì€ ë°°ê²½ (íˆ¬ëª…ë„ ë†’ì„)
        overlay = Image.new('RGBA', frame_pil.size, (0, 0, 0, 0))
        overlay_draw = ImageDraw.Draw(overlay)
        overlay_draw.rectangle(bg_rect, fill=(0, 0, 0, 100), outline=(255, 255, 255, 150))
        frame_pil = Image.alpha_composite(frame_pil.convert('RGBA'), overlay).convert('RGB')
        
        # ë°°ê²½ ì ìš© í›„ ë‹¤ì‹œ draw ê°ì²´ ìƒì„±
        draw = ImageDraw.Draw(frame_pil)
        
        # ì œëª© (ì›ë˜ í¬ê¸°ë¡œ)
        title_text = "ğŸŒ ë‹¤êµ­ì–´ ìë§‰"
        title_font = self.get_korean_font(size=20)  # ì›ë˜ í¬ê¸°
        if title_font:
            draw.text((subtitle_x, 20), title_text, fill=(255, 255, 0), font=title_font)
        else:
            draw.text((subtitle_x, 20), title_text, fill=(255, 255, 0))
        
        # ìë§‰ í‘œì‹œ (ìµœê·¼ max_linesê°œë§Œ)
        max_lines_adjusted = min(max_lines, (bg_height - 80) // 30)  # ì›ë˜ ì¤„ê°„ê²©ìœ¼ë¡œ ì¡°ì •
        recent_lines = subtitle_lines[-max_lines_adjusted:] if len(subtitle_lines) > max_lines_adjusted else subtitle_lines
        
        y_offset = 60  # ì›ë˜ ìœ„ì¹˜
        line_height = 30  # ì›ë˜ ì¤„ ê°„ê²©
        
        # ì›ë˜ í°íŠ¸ í¬ê¸°ë¡œ ìë§‰ í‘œì‹œ (ì•ì— - ì¶”ê°€)
        subtitle_font = self.get_korean_font(size=20)  # ì›ë˜ í¬ê¸°
        for i, line in enumerate(recent_lines):
            display_line = f"- {line}"  # ì•ì— - ì¶”ê°€
            if subtitle_font:
                draw.text((subtitle_x, y_offset + i * line_height), display_line, fill=(255, 255, 255), font=subtitle_font)
            else:
                draw.text((subtitle_x, y_offset + i * line_height), display_line, fill=(255, 255, 255))
        
        # OpenCV í˜•ì‹ìœ¼ë¡œ ë‹¤ì‹œ ë³€í™˜
        frame_with_subtitle = cv2.cvtColor(np.array(frame_pil), cv2.COLOR_RGB2BGR)
        return frame_with_subtitle
    
    def process_video(self, video_path, output_path, chunk_duration=25):
        """
        ë¹„ë””ì˜¤ì— STT ìë§‰ ì¶”ê°€
        """
        print(f"ğŸ¬ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘: {video_path}")
        
        # 1. ì˜¤ë””ì˜¤ ì¶”ì¶œ
        temp_audio = tempfile.mktemp(suffix='.wav')
        audio_path = self.extract_audio_with_ffmpeg(video_path, temp_audio)
        if not audio_path:
            return False
        
        # 2. ì˜¤ë””ì˜¤ ì²­í¬ ë¶„í• 
        chunks = self.split_audio_chunks(audio_path, chunk_duration)
        if not chunks:
            print("âŒ ì˜¤ë””ì˜¤ ì²­í¬ ë¶„í•  ì‹¤íŒ¨")
            return False
        
        # 3. STT ì²˜ë¦¬ (ë¬¸ì¥ë³„ íƒ€ì„ìŠ¤íƒ¬í”„)
        print("ğŸ™ï¸ ìŒì„± ì¸ì‹ ì¤‘ (ë¬¸ì¥ë³„ ë¶„ë¦¬)...")
        all_sentences = []
        
        for i, (chunk_path, start_time, end_time) in enumerate(chunks):
            print(f"ğŸ“ ì²­í¬ {i+1}/{len(chunks)} ì²˜ë¦¬ ì¤‘... ({start_time:.1f}~{end_time:.1f}ì´ˆ)")
            sentences = self.transcribe_chunk(chunk_path, start_time)
            
            if sentences:
                all_sentences.extend(sentences)
                for sentence in sentences:
                    print(f"âœ… ìŒì„±: [{sentence['speech_start']:.1f}~{sentence['speech_end']:.1f}s] â†’ ìë§‰: [{sentence['timestamp']:.1f}s] {sentence['text']}")
            else:
                print("âšª ìŒì„± ì—†ìŒ")
        
        # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
        all_sentences.sort(key=lambda x: x['timestamp'])
        
        # 4. ë¹„ë””ì˜¤ì— ìë§‰ ì¶”ê°€ (ì‹¤ì‹œê°„ ìŠ¤íƒ€ì¼)
        print("ğŸ¥ ë¹„ë””ì˜¤ì— ì‹¤ì‹œê°„ ìŠ¤íƒ€ì¼ ìë§‰ ì¶”ê°€ ì¤‘...")
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # ë¹„ë””ì˜¤ ë¼ì´í„° ì„¤ì • (ìŒì„± ì—†ì´ ì„ì‹œ íŒŒì¼)
        temp_video = tempfile.mktemp(suffix='_temp.mp4')
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(temp_video, fourcc, fps, (width, height))
        
        subtitle_lines = []
        added_sentences = set()  # ì´ë¯¸ ì¶”ê°€ëœ ë¬¸ì¥ ì¶”ì 
        frame_count = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # í˜„ì¬ ì‹œê°„ ê³„ì‚°
            current_time = frame_count / fps
            
            # ì•„ì§ ì¶”ê°€ë˜ì§€ ì•Šì€ ë¬¸ì¥ ì¤‘ì—ì„œ ì‹œê°„ì´ ì§€ë‚œ ê²ƒë“¤ ì¶”ê°€
            for sentence in all_sentences:
                sentence_id = f"{sentence['timestamp']:.1f}_{sentence['text']}"
                
                # í•´ë‹¹ ë¬¸ì¥ì˜ ì‹œì‘ ì‹œê°„ì´ ì§€ë‚¬ê³ , ì•„ì§ ì¶”ê°€ë˜ì§€ ì•Šì•˜ë‹¤ë©´
                if current_time >= sentence['timestamp'] and sentence_id not in added_sentences:
                    subtitle_lines.append(f"{sentence['text']}")
                    added_sentences.add(sentence_id)
                    print(f"ğŸ“º [{current_time:.1f}s] ìë§‰ í‘œì‹œ: {sentence['text']} (ìŒì„±: {sentence['speech_start']:.1f}~{sentence['speech_end']:.1f}s)")
            
            # ìë§‰ì´ í¬í•¨ëœ í”„ë ˆì„ ìƒì„±
            frame_with_subtitle = self.add_subtitle_to_frame(frame, subtitle_lines)
            out.write(frame_with_subtitle)
            
            frame_count += 1
            
            # ì§„í–‰ë¥  í‘œì‹œ
            if frame_count % (fps * 10) == 0:  # 10ì´ˆë§ˆë‹¤
                progress = (frame_count / total_frames) * 100
                print(f"â³ ì§„í–‰ë¥ : {progress:.1f}%")
        
        cap.release()
        out.release()
        
        # 5. ì›ë³¸ ìŒì„±ê³¼ ìë§‰ ë¹„ë””ì˜¤ í•©ì¹˜ê¸°
        print("ğŸ”Š ì›ë³¸ ìŒì„±ê³¼ ìë§‰ ë¹„ë””ì˜¤ í•©ì¹˜ëŠ” ì¤‘...")
        merge_command = [
            'ffmpeg', '-i', temp_video,  # ìë§‰ì´ í¬í•¨ëœ ë¹„ë””ì˜¤ (ìŒì„± ì—†ìŒ)
            '-i', video_path,  # ì›ë³¸ ë¹„ë””ì˜¤ (ìŒì„± í¬í•¨)
            '-c:v', 'copy',  # ë¹„ë””ì˜¤ ì½”ë± ë³µì‚¬ (ë¹ ë¥¸ ì²˜ë¦¬)
            '-c:a', 'aac',   # ì˜¤ë””ì˜¤ ì½”ë± AAC
            '-map', '0:v:0',  # ì²« ë²ˆì§¸ ì…ë ¥ì˜ ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼
            '-map', '1:a:0',  # ë‘ ë²ˆì§¸ ì…ë ¥ì˜ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼
            '-shortest',  # ì§§ì€ ìª½ì— ë§ì¶¤
            '-y',  # ë®ì–´ì“°ê¸°
            output_path
        ]
        
        try:
            subprocess.run(merge_command, capture_output=True, text=True, check=True)
            print("âœ… ìŒì„±ê³¼ ìë§‰ í•©ì¹˜ê¸° ì™„ë£Œ!")
        except subprocess.CalledProcessError as e:
            print(f"âš ï¸ ìŒì„± í•©ì¹˜ê¸° ì‹¤íŒ¨, ëŒ€ì²´ ë°©ë²• ì‹œë„...")
            print(f"ì˜¤ë¥˜: {e.stderr}")
            
            # ëŒ€ì²´ ë°©ë²•: ë” í˜¸í™˜ì„± ë†’ì€ ëª…ë ¹ì–´
            fallback_command = [
                'ffmpeg', '-i', temp_video,
                '-i', video_path,
                '-c:v', 'libx264',  # H.264 ì½”ë± ì‚¬ìš©
                '-c:a', 'aac',
                '-map', '0:v',  # ìë§‰ ë¹„ë””ì˜¤ì˜ ì˜ìƒ
                '-map', '1:a',  # ì›ë³¸ì˜ ìŒì„±
                '-shortest',
                '-y',
                output_path
            ]
            
            try:
                subprocess.run(fallback_command, capture_output=True, text=True, check=True)
                print("âœ… ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ìŒì„±ê³¼ ìë§‰ í•©ì¹˜ê¸° ì™„ë£Œ!")
            except subprocess.CalledProcessError as e2:
                print(f"âŒ ìŒì„± í•©ì¹˜ê¸° ìµœì¢… ì‹¤íŒ¨: {e2.stderr}")
                # ìë§‰ë§Œ ìˆëŠ” ë¹„ë””ì˜¤ë¼ë„ ì €ì¥
                import shutil
                shutil.move(temp_video, output_path)
                print(f"âš ï¸ ìŒì„± ì—†ì´ ìë§‰ë§Œ ì €ì¥ë¨: {output_path}")
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        try:
            os.unlink(temp_audio)
            os.unlink(temp_video)
            for chunk_path, _, _ in chunks:
                os.unlink(chunk_path)
        except:
            pass
        
        print(f"ğŸ‰ ì™„ë£Œ! ìŒì„±ê³¼ ìë§‰ì´ í¬í•¨ëœ íŒŒì¼: {output_path}")
        return True

def main():
    """
    ë©”ì¸ í•¨ìˆ˜
    """
    parser = argparse.ArgumentParser(
        description="FaceMaker STT with Whisper-Large-V3",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python whisper_v3_stt.py -i cctv.mp4 -o cctv_with_subtitle.mp4
  python whisper_v3_stt.py -i cctv.mp4 -o cctv_with_subtitle.mp4 --chunk 20
        """
    )
    
    parser.add_argument(
        '-i', '--input', 
        required=True, 
        help='ì…ë ¥ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ'
    )
    
    parser.add_argument(
        '-o', '--output', 
        required=True, 
        help='ì¶œë ¥ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ'
    )
    
    parser.add_argument(
        '--chunk', 
        type=int, 
        default=25, 
        help='ì˜¤ë””ì˜¤ ì²­í¬ ê¸¸ì´ (ì´ˆ, ê¸°ë³¸ê°’: 25 - ì •í™•ë„ì™€ ì†ë„ ê· í˜•)'
    )
    
    args = parser.parse_args()
    
    # ì…ë ¥ íŒŒì¼ í™•ì¸
    if not os.path.exists(args.input):
        print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input}")
        return 1
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(os.path.dirname(args.output) if os.path.dirname(args.output) else '.', exist_ok=True)
    
    print("=" * 60)
    print("ğŸ¯ FaceMaker STT with Whisper-Large-V3")
    print(f"ğŸ“ ì…ë ¥: {args.input}")
    print(f"ğŸ’¾ ì¶œë ¥: {args.output}")
    print(f"â±ï¸ ì²­í¬ ê¸¸ì´: {args.chunk}ì´ˆ")
    print("=" * 60)
    
    try:
        stt = WhisperV3STT()
        success = stt.process_video(args.input, args.output, args.chunk)
        
        if success:
            print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            return 0
        else:
            print("\nâŒ ì‘ì—… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            return 1
            
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main()) 